{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "import datetime\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import sympy as sp\n",
    "# import gurobipy as gp\n",
    "\n",
    "from openai import OpenAI, AsyncClient\n",
    "from json import JSONDecodeError\n",
    "from tqdm.auto import tqdm\n",
    "from colorama import Fore, Style\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from llama_index.core.program import LLMTextCompletionProgram\n",
    "from llama_index.llms.lmstudio import LMStudio\n",
    "\n",
    "sys.path.append('../')\n",
    "from utils import *\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-20 17:25:22.742\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mread_txt_file\u001b[0m:\u001b[36m15\u001b[0m - \u001b[34m\u001b[1mReading file: ../data\\LPWP\\LPWP.txt\u001b[0m\n",
      "\u001b[32m2024-09-20 17:25:22.745\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mread_txt_file\u001b[0m:\u001b[36m17\u001b[0m - \u001b[34m\u001b[1mFile read successfully: ../data\\LPWP\\LPWP.txt\u001b[0m\n",
      "\u001b[32m2024-09-20 17:25:22.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mget_nl4opt_qas\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mNumber of questions: 288\u001b[0m\n",
      "\u001b[32m2024-09-20 17:25:22.748\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mget_nl4opt_qas\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mNumber of answers: 288\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data'\n",
    "DATASET_NAME = 'LPWP' \n",
    "OUTPUT_DIR = '../output'  \n",
    "\n",
    "dt = datetime.datetime.today().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "\n",
    "nl4opt_data = read_txt_file(os.path.join(DATA_DIR, DATASET_NAME, 'LPWP.txt'))\n",
    "questions, answers = get_nl4opt_qas(nl4opt_data)\n",
    "assert len(questions) == len(answers)\n",
    "\n",
    "qa_pairs = list(zip(questions, answers))\n",
    "# demo_samples, test_samples = get_demo_and_test_samples(qa_pairs)\n",
    "\n",
    "questions = [q for q, _ in qa_pairs]\n",
    "answers = [a for _, a in qa_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LMStudio(\n",
    "    model_name=\"lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF\",\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in optimization problems. Your task is to find the optimal solution to the given problem. If there is no optimial solution, please return inf. Guide the user through the solution step by step.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_template_str = \"\"\"You are an expert in optimization problems. Your task is to find the optimal solution to the given problem. If there is no optimial solution, please return inf. Guide the user through the solution step by step.\\n\"\"\"\n",
    "print(prompt_template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_result_list = []\n",
    "for i in tqdm(range(len(questions))):\n",
    "  response = client.chat.completions.create(\n",
    "        model=\"lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF\",\n",
    "        messages=[ \n",
    "          { \"role\": \"system\", \"content\": prompt_template_str },\n",
    "          { \"role\": \"user\", \"content\": f\"QUESTION: {questions[i]}\" }\n",
    "        ], \n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "            \"strict\": \"true\",\n",
    "            \"schema\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                \"rea\": {\n",
    "                  \"type\": \"string\",\n",
    "                  \"description\": \"The reasoning process to find the optimal solution.\"\n",
    "                },\n",
    "                \"res\": {\n",
    "                  \"type\": \"number\",\n",
    "                  \"description\": \"The final optimal answer to the question.\"\n",
    "                }\n",
    "              },\n",
    "            \"required\": [\"res\"]\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        temperature=0.7, \n",
    "        max_tokens=-1,\n",
    "        stream=False,\n",
    "    )\n",
    "  \n",
    "  lp_result_list.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_results = []\n",
    "for lp_r in lp_result_list:\n",
    "    lp_json = json.loads(lp_r)\n",
    "    lp_results.append(lp_json['res'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'llama3.1_8B_lpwp_gurobi_baseline_' + dt + '.pkl'\n",
    "with open(os.path.join(OUTPUT_DIR, filename), 'wb') as f:\n",
    "    pickle.dump(lp_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'llama3.1_8B_lpwp_gurobi_baseline_2024-09-20-17-25-22.pkl'\n",
    "with open(os.path.join(OUTPUT_DIR, filename), 'rb') as f:\n",
    "    pred_answers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [a if a != 'None' else str(np.inf) for a in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 3.6, 2.5, 2, 0, -1, 0, 0, 0, 1.0, -1, -1, 1, 0, 10.0, -1, 0, -1, 0, 3, 5000, 0, 8500.0, 1, 28.0, 3000, 80, 1, 84.0, 1, 0, 3000, 0, 100, 0, 25.0, 2, 300.0, 0, 1, -300, 1500, 0, 240.0, 0, 0, 4, 0, 0, 1000, 0.0, 1, 1.2, 0, 2700, 0, 500, -1, 0, 1.0, 1.1, 1600, 0, 7, 0, 1.2, 0, 0, 0, 1, 90, 0, 3500, 1, 1, 0, 0, 1, -1e+308, 2800.0, 40.0, 0, 0, 25, 0, 3000.0, 6, 0, 4.0, 0.0, 2000, 0, 3.0, 0, 0, -1, 0, 29, 0, 7, 1, 0, 0, 1, 0, 0, 1, 1, 25, 0, 0.0, 1.5, 1.0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 20, 0, 1, 0, 0.5, 42000, 1, 1, 0, 0, 89.0, 1.1, 0.0, 0, 0, 5.5, -1, 0, 0, 0, 0, -1, 1, 0.0, 5, 1, 3.1, 1, 1, 20, -1, 0, 150, 0, 0, 5000.0, 85, 0, 0, 0, -1, 4, 90, 1, 0, 0, 18000, 0, 17.0, 18, 0, 40.0, -1, 0, 1, 0, 1, 4, 1.0, 0, 1518.0, 1, 0, 0, 4, 0.0, 1.1, 0.0, 0, 1, 0.0, 1, 5, 4200, 0, 500.0, 3000, 0, 1, 0, 0, 10, -1, 3600000.0, 27.0, 1.1, 0, 0, 2, 25000.0, 0.0, 1.5, 0, 8.0, 26, 10.0, 1, 0, 0.0, 5, 0, 0, 1.0, 1, 0, 1, 15.0, 0.0, 1, 0, 0, -1, 30, -1, 1500.0, 1.25, 160.0, 0, 1, -1, 1, 84000.0, 1, 1, 1, -1, 0, -1, 0, 10, 0, 1.6, 3000, 4.0, 14, 5, 1, 1, 300, 0, 0, 1, 1500.0, 1, 1.5, 23, 1.2, 0, 8, 0, -1, 2, -1, 1, -1, 0, 0, -1, 1, 2, -1.0, 3.5, 0, 500, 0]\n"
     ]
    }
   ],
   "source": [
    "print(pred_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1300', '5050', '125.4929565', '3', '110.0', '45.0', '810.0', '582.0', '1400.0', '36.0', '96', '0', '0.0', '684000.0', '4.0', '64', '125.0', '2500', '98.0', '62.5', '0.0', '160', '226.0', '5625', '60', '1.5', '750.0', '200', '735.0', '360', '368', '0', '603.0', '1000.0', '4347.82612', '899975', '20.8', '440.0', '60', '1266', '2000', '87.5', '1500.0', '105', '68.0', 'inf', '1140', '1.5', '72', '50.0', '350', '833.3333299999999', '0.0', '390', '40', '571.0', '24.0', '0.0', '1990', '6300.0', '1160.0', '2200', '2480', '71.0', '14.0', '60', '20.0', '75.0', '4000000.0', '465.0', '580.0', '555', '120.0', '1970.0', '513', '16.0', '67.0', '75', '19.0', '224.0', '4190.0', '600.0', '33', '150', '540', '342.0', 'inf', '19.0', '80', '26.0', '327.65957199999997', '1480.0', '310.0', '430.0', '175.0', '960', '2500', '29.0', '32', '78', '45', '239.0', '16500', '230.0', '24', '670', '420.0', '841.0', 'inf', '89.0', '37.0', '23', '150000.0', '14', '342750', '133200', '11980', '30000.0', '3.0', '20.0', '760', '290.5', '175.0', '70', '9000.0', '36900.0', '150000.0', '16.0', '85500.0', '0.72', '60000', '648.0', '535', '217.0', '4990.0', '136.0', '0.0', '256500.0', '1070.0', '42000', '0', '52', '84.0', '300.0', '25', '18', '500.0', '690.0', '670.0', '1080.0', '25.0', '16666.6668', '291.4', '1965', '65.0', '10060', '30.0', '480', '0', '95.0', '460.0', '0.0', '72', '495.0', '990', '0', '0.0', '6', '80000', 'inf', '46', '35.0', '26.0', '620.0', '0', '11.0', '98', '2250', '100.0', '7.0', 'inf', '3163.26532', '610', '0', '16666.667', '29.0', '0.0', '333.0', '0', '310', '310.0', '56.0', '1000.0', '214.0', '136', '3200', '61875.0', '0.0', '136.363633', '19', '0.0', '2400', '14.0', '210.0', '1001.0', '40', '0.0', '0', '0', '12600', '511.42857200000003', '166', '7000.0', '80000', '81000', '40.0', '5.85185186', '50', '540', '650.0', '200.0', '4400', '2190.0', '29950.0', '215000.0', '1366.6666559999999', '22.0', '400000', '14325.0', '30.0', '1060', '1500', '206250.0', '2400.0', '268.0', '22.0', '8.0', '750', '0', '1800.0', '6794.0', 'inf', '70.0', '0', '79000.0', '35.0', '17.0212768', '0', '7', '950.0', '1125.0', '0', '6', '100.0', '0', '6000.0', 'inf', '67', '0.0', '60', '369.0', '17000', '0', '8', '0.0', '0.0', '225.00000000000003', '40.0', '18.0', '8', '22.0', '1680.0', '0', '0.0', '7.5', '363.0', '0', '507', '100', '28', '5.0', '0.0', '1552', '30.0', '0', '7.0', '142', '0']\n"
     ]
    }
   ],
   "source": [
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark(pred, real, error: float) -> List[bool]:    \n",
    "    correct = []\n",
    "    for p, r in zip(pred, real):\n",
    "        if p == 'Error':\n",
    "            continue\n",
    "        if float(r) != 0:\n",
    "            if (float(p) == np.inf and float(r) == np.inf) or (abs(float(p) - float(r)) / float(r) < error):\n",
    "                correct.append(True)\n",
    "            else:\n",
    "                correct.append(False)\n",
    "        else:\n",
    "            if float(p) < error:\n",
    "                correct.append(True)\n",
    "            else:\n",
    "                correct.append(False)\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy under error 0.01: 5.555555555555555\n",
      "Accuracy under error 0.0001: 5.555555555555555\n",
      "Accuracy under error 1e-06: 5.555555555555555\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy under error {1e-2}: {sum(mark(pred_answers, answers, 1e-2)) / len(answers) * 100}\")\n",
    "print(f\"Accuracy under error {1e-4}: {sum(mark(pred_answers, answers, 1e-4)) / len(answers) * 100}\")\n",
    "print(f\"Accuracy under error {1e-6}: {sum(mark(pred_answers, answers, 1e-6)) / len(answers) * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "or",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
