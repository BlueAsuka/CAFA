{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HAOXUAN\\miniconda3\\envs\\or\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "import asyncio\n",
    "import pickle\n",
    "import datetime\n",
    "sys.path.append('../')\n",
    "\n",
    "from openai import OpenAI, AsyncClient\n",
    "from json import JSONDecodeError\n",
    "from tqdm.auto import tqdm\n",
    "from utils import *\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = json.load(open('../configs./configs.json', 'r'))\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "asyncclient = AsyncClient(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "dt = datetime.datetime.today().strftime('%Y-%m-%d-%H-%M-%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-17 12:28:25.065\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mread_txt_file\u001b[0m:\u001b[36m14\u001b[0m - \u001b[34m\u001b[1mReading file: ../data\\NL4OPT\\nl4opt.txt\u001b[0m\n",
      "\u001b[32m2024-09-17 12:28:25.066\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mread_txt_file\u001b[0m:\u001b[36m16\u001b[0m - \u001b[34m\u001b[1mFile read successfully: ../data\\NL4OPT\\nl4opt.txt\u001b[0m\n",
      "\u001b[32m2024-09-17 12:28:25.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mget_nl4opt_qas\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mNumber of questions: 245\u001b[0m\n",
      "\u001b[32m2024-09-17 12:28:25.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mget_nl4opt_qas\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mNumber of answers: 245\u001b[0m\n",
      "\u001b[32m2024-09-17 12:28:25.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mget_demo_and_test_samples\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mNumber of demo samples: 20\u001b[0m\n",
      "\u001b[32m2024-09-17 12:28:25.069\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mget_demo_and_test_samples\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mNumber of test samples: 225\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data'\n",
    "DATASET_NAME = 'NL4OPT' \n",
    "OUTPUT_DIR = '../output'  \n",
    "\n",
    "nl4opt_data = read_txt_file(os.path.join(DATA_DIR, DATASET_NAME, 'nl4opt.txt'))\n",
    "questions, answers = get_nl4opt_qas(nl4opt_data)\n",
    "assert len(questions) == len(answers)\n",
    "\n",
    "qa_pairs = list(zip(questions, answers))\n",
    "demo_samples, test_samples = get_demo_and_test_samples(qa_pairs)\n",
    "\n",
    "questions = [q for q, _ in test_samples]\n",
    "answers = [a for _, a in test_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-4o-mini + Zero-Shot CoT + Structured Output = 21.33%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:16<00:00, 13.10s/it]\n"
     ]
    }
   ],
   "source": [
    "class Step(BaseModel):\n",
    "    explanation: str\n",
    "    output: str\n",
    "\n",
    "class LPReasoning(BaseModel):\n",
    "    steps: list[Step]\n",
    "    final_answer: float\n",
    "\n",
    "batch_size = 16\n",
    "sys_prompt = \"\"\"You are an expert in optimization problems. Your task is to find the optimal solution to the given problem. If there is no optimial solution, please return inf. Guide the user through the solution step by step.\"\"\"\n",
    "\n",
    "lp_reasoning_list = []\n",
    "for idx in tqdm(range(0, len(questions), batch_size)):\n",
    "    batch = questions[idx:idx+batch_size]\n",
    "    \n",
    "    tasks = [asyncclient.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0,\n",
    "        response_format=LPReasoning,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Problem: {q}\"}\n",
    "        ]) for q in batch\n",
    "    ]\n",
    "\n",
    "    combined_responses = await asyncio.gather(*tasks)\n",
    "    lp_reasoning_list.extend([r.choices[0].message.parsed for r in combined_responses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pickle to save the list\n",
    "filename = 'lp_reasoning_list-' + dt + '.pkl'\n",
    "with open(os.path.join(OUTPUT_DIR, filename), 'wb') as f:\n",
    "    pickle.dump(lp_reasoning_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-17 12:31:59.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSuccessfully parsed all predictions.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "error_idx = []\n",
    "pred_answers = []\n",
    "all_pass = True\n",
    "for i, lr in enumerate(lp_reasoning_list):\n",
    "    try:\n",
    "        pred_answers.append(lr.final_answer)\n",
    "    except:\n",
    "        all_pass = False\n",
    "        error_idx.append(i)\n",
    "        continue\n",
    "\n",
    "if all_pass:\n",
    "    loguru.logger.info(f\"Successfully parsed all predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(pred_answers) == len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = []\n",
    "for p, r in zip(pred_answers, answers):\n",
    "    if (float(p) == np.inf and float(r) == np.inf) or (abs(float(p) - float(r)) / float(r) < 1e-2):\n",
    "        correct.append(True)\n",
    "    else:\n",
    "        correct.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.333333333333336"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(correct) / len(answers)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
